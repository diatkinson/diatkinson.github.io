---
---
@inproceedings{atkinson-etal-2019-gets,
    title = "What Gets Echoed? Understanding the {``}Pointers{''} in Explanations of Persuasive Arguments",
    author = "Atkinson, David  and
      Srinivasan, Kumar Bhargav  and
      Tan, Chenhao",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1289",
    arxiv={1911.00523},
    doi = "10.18653/v1/D19-1289",
    pages = "2911--2921",
    abstract = "Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.",
}

@inproceedings{naihin2023testing,
      title={Testing Language Model Agents Safely in the Wild}, 
      author={Silen Naihin and David Atkinson and Marc Green and Merwane Hamadi and Craig Swift and Douglas Schonholtz and Adam Tauman Kalai and David Bau},
      booktitle = "Socially Responsible Language Modelling Research (SoLaR) workshop at NeurIPS 2023",
      year={2023},
      month = nov,
      arxiv={2311.10538},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      abstract = "A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We design a basic safety monitor (AgentMonitor) that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the AgentMonitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable."
}